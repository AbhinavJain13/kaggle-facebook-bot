{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction\n",
    "\n",
    "In this Jupyter notebook, I present my approach to the recent Kaggle competition, [Facebook Recruiting IV: Human or Robot?](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot). The main idea was to treat the modeling problem as somewhat of a text classification problem by sorting the bid data for each bidder in chronological order and using this sorted bid data as a text-based \"fingerprint\" of the bidder's activities. In addition to this, I computed some numerical features for the time differences between bids and the unique counts of the different entries. The final model ended up being a bag of 15 XGBoost models on a sparse matrix of tfidf-vectorized text features concatenated with scaled numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "For the most part, I used Python standard libraries and the scientific Python libraries available in the Anaconda distribution (`pandas`, `scikit-learn`, `scipy` and `numpy`). The only slightly more exotic library is `XGBoost`. For installing `XGBoost` on Windows, [these](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13043/run-xgboost-from-windows-and-python) instructions by Alejandro Simkievich and [this](https://github.com/dmlc/xgboost/tree/master/windows) repository on `XGBoost`'s Github profile were helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "First, we read the bid data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bids = pd.read_csv('data/bids.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we will tokenize the bid information on spaces, so we remove any additional spaces from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bids = df_bids.replace({' ': ''}, regex = True) #remove spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of the approach is dealing with the bids in chronological order. Hence, we sort the bids in ascending order of bidder_ids and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bids_sorted = df_bids.sort(['bidder_id', 'time'], ascending = [True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a `bids`-dataframe where the aggregated bid information for each bidder_id will be gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe for aggregated bid data\n",
    "bids = pd.DataFrame(data = df_bids_sorted['bidder_id'].unique(), columns = ['bidder_id'],\n",
    "                    index = df_bids_sorted['bidder_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first entry into the `bids`-dataframe, we count the number of auctions for each bidder_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#auction counts                  \n",
    "counts = df_bids_sorted.groupby('bidder_id')['bidder_id'].agg('count')\n",
    "bids['auction_count_num'] = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute the time differences between bids into the `df_bids_sorted`-dataframe. These time differences are included in both numeric and string form. I noticed that there were some time differences that occur quite frequently, and a text processing of time differences should be able to identify these types of patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timediff = df_bids_sorted.groupby('bidder_id')['time'].diff()\n",
    "timediff_str = timediff.astype(str).fillna('')\n",
    "df_bids_sorted['timediff_num'] = timediff\n",
    "df_bids_sorted['timediff'] = timediff_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the main aggregation step for the bid data is performed. For each column, the data is first converted into slightly more readable form. For example for urls the data is transformed from the form `0esea7scvgr82he` to the form `url_0esea7scvgr82he`. This was done to make the different entries more identifiable in the case of evaluating feature importances, but in the end this was not utilized to any significant extent. The entries for each bidder were concatenated with a space-delimiter to generate the aggregated text data. The result is a long string of space-delimited entries for each column. E.g. for the device-column, we can have a string of the type `device_phone167 device_phone172 device_phone167` etc. \n",
    "\n",
    "In addition to generating the concatenated text data, the number of unique entries was also computed for each column and bidder. Throughout this notebook, we add a `_text`-suffix to text columns and a `_num`-suffix to numerical columns. This allows for the two types of columns to be selected with regular expressions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn feature sequences into text\n",
    "text_cols = ['auction', 'merchandise', 'device', 'timediff', 'country', 'ip', 'url']\n",
    "for var in text_cols:\n",
    "    df_bids_sorted[var] = var + \"_\" + df_bids_sorted[var].fillna(\"\")\n",
    "    text_str = var + '_text'\n",
    "    count_str = var + '_nunique_num'\n",
    "    bids[text_str] = df_bids_sorted.groupby('bidder_id')[var].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "    bids[count_str]  = df_bids_sorted.groupby('bidder_id')[var].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One idea I had was that the distribution of time differences between bids could be a significant predictor of bot activity. Hence, I computed a number of different descriptive statistics related to the times and time differences. In the following, the min and max times and time differences are computed along with the time difference range, mean, median and the first to ninth deciles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_time = df_bids_sorted.groupby('bidder_id')['time'].max()\n",
    "bids['maxtime_num'] = max_time\n",
    "min_time = df_bids_sorted.groupby('bidder_id')['time'].min()\n",
    "bids['mintime_num'] = min_time\n",
    "max_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].max()\n",
    "max_diff = max_diff.fillna(max_diff.mean())\n",
    "bids['maxdiff_num'] = max_diff\n",
    "min_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].max()\n",
    "min_diff = min_diff.fillna(min_diff.mean())\n",
    "bids['mindiff_num'] = min_diff\n",
    "range_diff = max_diff - min_diff\n",
    "bids['rangediff_num'] = range_diff\n",
    "mean_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].mean()\n",
    "mean_diff = mean_diff.fillna(mean_diff.mean())\n",
    "bids['meandiff_num'] = mean_diff\n",
    "median_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].median()\n",
    "median_diff = median_diff.fillna(median_diff.mean())\n",
    "bids['mediandiff_num'] = median_diff\n",
    "for q in [0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]:\n",
    "    q_string = 'diff_quantile_num_' + str(q).replace('.', '_')\n",
    "    q_temp = df_bids_sorted.groupby('bidder_id')['timediff_num'].quantile(q)\n",
    "    q_temp = q_temp.fillna(q_temp.mean())\n",
    "    bids[q_string] = q_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the bid aggregation step. After this, we read the `train`- and `test`-data and merge it with the bid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_combo = df_train.append(df_test)\n",
    "df_combo['address_text'] = 'address_' + df_combo['address'].fillna('')\n",
    "df_combo['account_text'] = 'account_' + df_combo['payment_account'].fillna('')\n",
    "df_combo = df_combo.merge(bids, how = 'left', left_on = ['bidder_id'], right_on = ['bidder_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the redundant dataframes and run garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df_train\n",
    "del df_test\n",
    "del df_bids\n",
    "del df_bids_sorted\n",
    "del bids\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regular expressions, the text and numeric columns are identified. For missing values in numeric columns, we fill in the column mean; for missing values in text columns, we fill in an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cols = filter(re.compile('num').search, df_combo.columns)\n",
    "text_cols = filter(re.compile('text').search, df_combo.columns)\n",
    "for col in num_cols:\n",
    "    df_combo[col] = df_combo[col].fillna(df_combo[col].mean())\n",
    "for col in text_cols:\n",
    "    df_combo[col] = df_combo[col].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the `df_combo`-dataframe into train and test data. First, we grab the columns for the test data and check that the order matches that in the sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv('submissions/sampleSubmission.csv')\n",
    "test_dat = df_combo[df_combo.bidder_id.isin(sample.bidder_id)]\n",
    "#test\n",
    "print (sample.bidder_id.values==test_dat['bidder_id'].values).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the numeric columns into matrices `xtrain` and `xtest`. The processed sparse text frequency matrices will then be concatenated to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dat = df_combo[~pd.isnull(df_combo.outcome)]\n",
    "y = train_dat.outcome.values\n",
    "xtrain = train_dat[num_cols].values\n",
    "xtest = test_dat[num_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scaled using `scikit-learn`'s MinMaxScaler. We use the MinMaxScaler as it leads to non-negative values, which is useful later on as `scikit-learn`'s `chi2`-feature selection function only works on non-negative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = MinMaxScaler().fit(np.vstack((xtrain, xtest)))\n",
    "xtrain = sc.transform(xtrain)\n",
    "xtest = sc.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step before text processing, the numeric `xtrain`- and `xtest`-matrices are converted into sparse matrices to prepare for the concatenation with tfidf-matrices later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain = sparse.csr_matrix(xtrain)\n",
    "xtest = sparse.csr_matrix(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "The text columns are processed using the tfidf-vectorizer in `scikit-learn`. First we define a custom tokenizer-function that only split on spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens(x):\n",
    "    return x.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use slightly different tfidf-parameters for the different columns. These are stored in a `text_params`-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_params = {}\n",
    "text_params['address_text'] = {'include':False}\n",
    "text_params['account_text'] = {'include':False}\n",
    "text_params['auction_text'] = {'include':True, 'mindf':5, 'ngram':(1,3), 'token':'tokens'}\n",
    "text_params['merchandise_text'] = {'include':True, 'mindf':5, 'ngram':(1,3), 'token':'tokens'}\n",
    "text_params['device_text'] = {'include':True, 'mindf':5, 'ngram':(1,3), 'token':'tokens'}\n",
    "text_params['timediff_text'] = {'include':True, 'mindf':1, 'ngram':(1,1), 'token':'tokens'}\n",
    "text_params['country_text'] = {'include':True, 'mindf':5, 'ngram':(1,3), 'token':'tokens'}\n",
    "text_params['ip_text'] = {'include':True, 'mindf':1, 'ngram':(1,1), 'token':'nottokens'}\n",
    "text_params['url_text'] = {'include':True, 'mindf':5, 'ngram':(1,3), 'token':'tokens'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter choices were based on some brief univariate tests of `auc`-performance. The `address` and `account`-fields were not found to be very useful and were dropped from the data. \n",
    "\n",
    "For all columns except for the timediff, the ngrams parameter is (1,3), i.e. we also consider bigrams and trigrams in addition to single tokens. For these columns, we should thereby be able to identify some sequential structure in the bid data. \n",
    "\n",
    "Interestingly, the `ip_text`-column had an improved `auc` when using the default tfidf-tokenizer instead of our custom `tokens`-tokenizer. The default-tokenizer also splits on punctuation and thereby splits the ip into its sub-addresses, which turned out to be useful for this modeling problem. For the other text columns, we use the space-delimited `tokens`-tokenization.\n",
    "\n",
    "With the following code snippet, the text columns were vectorized and concatenated to the `xtrain`- and `xtest`-matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in text_cols:\n",
    "    if not text_params[col]['include']:\n",
    "        continue\n",
    "    else:\n",
    "        if text_params[col]['token'] == 'tokens':\n",
    "            vect = TfidfVectorizer(tokenizer = tokens, min_df = text_params[col]['mindf'],\n",
    "                                   ngram_range = text_params[col]['ngram'])\n",
    "        else:\n",
    "            vect = TfidfVectorizer(min_df = text_params[col]['mindf'],\n",
    "                                   ngram_range = text_params[col]['ngram'])\n",
    "        documents = df_combo[col].values\n",
    "        vect.fit(documents)\n",
    "        xtr_tmp = vect.transform(train_dat[col].values)\n",
    "        xte_tmp = vect.transform(test_dat[col].values)\n",
    "        xtrain = sparse.hstack((xtrain, xtr_tmp))\n",
    "        xtest = sparse.hstack((xtest, xte_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "For the final model, we perform a univariate `chi2`-feature selection to choose the top-25% of features. Then we fit 15 XGBoost-models to the data, where the only difference between the bagged models is the random seed. For the final submission, we take the average of the 15 models in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature selection\n",
    "feats_25 = SelectPercentile(chi2, 25).fit(xtrain, y)\n",
    "xtrain = feats_25.transform(xtrain)\n",
    "xtest = feats_25.transform(xtest)\n",
    "\n",
    "clf = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                            learning_rate = 0.05,\n",
    "                            max_depth = 5,\n",
    "                            nthread = 8,\n",
    "                            seed = 42,\n",
    "                            subsample = 0.4,\n",
    "                            colsample_bytree = 0.7,\n",
    "                            min_child_weight = 1,\n",
    "                            n_estimators = 100,\n",
    "                            gamma = 0.15, silent = True)\n",
    "\n",
    "#bag of 15 models\n",
    "rounds = 15\n",
    "preds_mat = np.zeros((len(sample.index), rounds))\n",
    "for i in range(rounds):\n",
    "    clf.set_params(seed = i + 1)\n",
    "    clf.fit(xtrain, y)\n",
    "    preds_tmp = clf.predict_proba(xtest)[:, 1]\n",
    "    preds_mat[:, i] = preds_tmp\n",
    "bagged_preds = preds_mat.mean(axis = 1)\n",
    "sample.prediction = bagged_preds\n",
    "sample.to_csv('submissions/facebook_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission should score around 0.93698 on the private leaderboard for a rank of 18th."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
